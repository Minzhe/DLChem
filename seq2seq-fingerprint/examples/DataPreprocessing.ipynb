{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data root (Output folder)\n",
    "import os\n",
    "\n",
    "# For import current library\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "expr_root = os.path.expanduser(\"~/expr/unsup-seq2seq/\")\n",
    "data_root = os.path.join(expr_root, \"data\")\n",
    "\n",
    "if not os.path.exists(data_root):\n",
    "    os.makedirs(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train/test data iter.\n",
    "\n",
    "train_data_path = \"/smile/nfs/projects/nih_drug/data/pm2/pm2.smi\"\n",
    "test_data_path = \"/smile/nfs/projects/nih_drug/data/logp/logp.smi\"\n",
    "\n",
    "def smi_data_iter(smi_path):\n",
    "    \"\"\"Yield logp SMILE representation.\"\"\"\n",
    "    with open(smi_path) as fobj:\n",
    "        for line in fobj:\n",
    "            if not len(line.strip()):\n",
    "                continue\n",
    "            _smile = line.strip().split()[0]\n",
    "            yield _smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write all smiles to files.\n",
    "\n",
    "def build_data_tmp(data_iter, data_path):\n",
    "    \"\"\"Build temp data file inside the data_directory. This is required for tensorflow function.\"\"\"\n",
    "    with open(data_path, \"w+\") as fobj:\n",
    "        for _smile in data_iter:\n",
    "            fobj.write(\"%s\\n\" % _smile)\n",
    "            \n",
    "train_smile_path = os.path.join(data_root, \"pm2.smiles\")\n",
    "test_smile_path = os.path.join(data_root, \"logp.smiles\")\n",
    " \n",
    "build_data_tmp(smi_data_iter(train_data_path), train_smile_path)\n",
    "build_data_tmp(smi_data_iter(test_data_path), test_smile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'#': 27,\n",
       "  '%': 37,\n",
       "  '(': 7,\n",
       "  ')': 6,\n",
       "  '+': 22,\n",
       "  '-': 24,\n",
       "  '.': 30,\n",
       "  '/': 25,\n",
       "  '0': 39,\n",
       "  '1': 10,\n",
       "  '2': 11,\n",
       "  '3': 12,\n",
       "  '4': 14,\n",
       "  '5': 21,\n",
       "  '6': 28,\n",
       "  '7': 32,\n",
       "  '8': 34,\n",
       "  '9': 36,\n",
       "  '=': 5,\n",
       "  '@': 20,\n",
       "  'Br': 26,\n",
       "  'C': 4,\n",
       "  'Cl': 19,\n",
       "  'F': 18,\n",
       "  'H': 23,\n",
       "  'I': 35,\n",
       "  'N': 9,\n",
       "  'O': 8,\n",
       "  'P': 31,\n",
       "  'S': 13,\n",
       "  '[': 15,\n",
       "  '\\\\': 17,\n",
       "  ']': 16,\n",
       "  '_EOS': 2,\n",
       "  '_GO': 1,\n",
       "  '_PAD': 0,\n",
       "  '_UNK': 3,\n",
       "  'c': 29,\n",
       "  'n': 33,\n",
       "  'o': 40,\n",
       "  's': 38},\n",
       " ['_PAD',\n",
       "  '_GO',\n",
       "  '_EOS',\n",
       "  '_UNK',\n",
       "  'C',\n",
       "  '=',\n",
       "  ')',\n",
       "  '(',\n",
       "  'O',\n",
       "  'N',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  'S',\n",
       "  '4',\n",
       "  '[',\n",
       "  ']',\n",
       "  '\\\\',\n",
       "  'F',\n",
       "  'Cl',\n",
       "  '@',\n",
       "  '5',\n",
       "  '+',\n",
       "  'H',\n",
       "  '-',\n",
       "  '/',\n",
       "  'Br',\n",
       "  '#',\n",
       "  '6',\n",
       "  'c',\n",
       "  '.',\n",
       "  'P',\n",
       "  '7',\n",
       "  'n',\n",
       "  '8',\n",
       "  'I',\n",
       "  '9',\n",
       "  '%',\n",
       "  's',\n",
       "  '0',\n",
       "  'o'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocabulary from train_data\n",
    "\n",
    "\n",
    "from unsupervised.utils import true_smile_tokenizer, get_vocabulary\n",
    "\n",
    "vocab_path = os.path.join(data_root, \"pm2.vocab\")\n",
    "\n",
    "get_vocabulary(train_smile_path, vocab_path, tokenizer=true_smile_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in /home/zhengxu/expr/unsup-seq2seq/data/pm2.smiles\n",
      "  tokenizing line 100000\n",
      "  tokenizing line 200000\n",
      "  tokenizing line 300000\n",
      "Tokenizing data in /home/zhengxu/expr/unsup-seq2seq/data/logp.smiles\n"
     ]
    }
   ],
   "source": [
    "from unsupervised.utils import true_smile_tokenizer, data_to_token_ids\n",
    "\n",
    "train_token_path = os.path.join(data_root, \"pm2.tokens\")\n",
    "test_token_path = os.path.join(data_root, \"logp.tokens\")\n",
    "\n",
    "data_to_token_ids(train_smile_path, train_token_path, vocab_path, true_smile_tokenizer)\n",
    "data_to_token_ids(test_smile_path, test_token_path, vocab_path, true_smile_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
