{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "from deepchem.feat import Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/zhengxu/github/drug/seq2seq-fingerprint/\")\n",
    "\n",
    "from unsupervised.seq2seq_model import FingerprintFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define our seq2seq featurizer.\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "class Seq2seqFeaturizer(Featurizer):\n",
    "    \"\"\"Seq2seq Featurizer.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dir, vocab_dir):\n",
    "        \"\"\"Define the seq2seq feature.\"\"\"\n",
    "        self.fetcher = FingerprintFetcher(model_dir, vocab_dir)\n",
    "        \n",
    "    def _featurize(self, mol):\n",
    "        \"\"\"\n",
    "        Calculate features for a single molecule.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mol : RDKit Mol\n",
    "            Molecule.\n",
    "        \"\"\"\n",
    "        # This is a bit hacky. I have no idea why we have to start from mol instead of original smile.\n",
    "        smile = Chem.MolToSmiles(mol)\n",
    "        fp, _ = self.fetcher.decode(smile)\n",
    "        return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading seq2seq model definition from /home/zhengxu/expr/test/gru-4-256/model.json...\n",
      "Loading model weights from checkpoint_dir: /home/zhengxu/expr/test/gru-4-256/weights/\n"
     ]
    }
   ],
   "source": [
    "# Initailize the featurizer and cache it.\n",
    "sess = tf.InteractiveSession()\n",
    "featurizer = Seq2seqFeaturizer(\"/home/zhengxu/expr/test/gru-4-256\", \"/home/zhengxu/expr/test/pretrain/pm2.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build up specific model builder.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "# Building scikit random forest model\n",
    "\n",
    "# Use this class to select different models for different task/dataset/split.\n",
    "class SKLearnModelSelector(object):\n",
    "    \n",
    "    DATASET_MAPPING = {\n",
    "        \"delaney\": {\n",
    "            \"index\": (RandomForestRegressor, {}),\n",
    "            \"random\": (RandomForestRegressor, {}),\n",
    "            \"scaffold\": (RandomForestRegressor, {})\n",
    "        },\n",
    "        \"sampl\": {\n",
    "            \"index\": (RandomForestRegressor, {}),\n",
    "            \"random\": (RandomForestRegressor, {}),\n",
    "            \"scaffold\": (RandomForestRegressor, {})\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, dataset, split):\n",
    "        \"\"\"Input dataset and split.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.split = split\n",
    "        \n",
    "    def __call__(self, task):\n",
    "        model_class, model_hparam = self.DATASET_MAPPING[self.dataset][self.split]\n",
    "        sklearn_model = model_class(**model_hparam)\n",
    "        return dc.models.sklearn_models.SklearnModel(sklearn_model, task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Dataset: delaney, split: index\n",
      "-------------------------------------\n",
      "Loading dataset: delaney\n",
      "-------------------------------------\n",
      "Splitting function: index\n",
      "About to featurize Delaney dataset.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/delaney-processed.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 50.006 s\n",
      "TIMING: dataset construction took 50.163 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 0.174 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.144 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.038 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.040 s\n",
      "Loading dataset from disk.\n",
      "About to initialize singletask to multitask model\n",
      "Initializing directory for task measured log solubility in mols per litre\n",
      "About to create task-specific datasets\n",
      "Splitting multitask dataset into singletask datasets\n",
      "TIMING: dataset construction took 0.001 s\n",
      "Loading dataset from disk.\n",
      "Processing shard 0\n",
      "\tTask measured log solubility in mols per litre\n",
      "Dataset for task measured log solubility in mols per litre has shape ((902, 1024), (902, 1), (902, 1), (902,))\n",
      "Fitting model for task measured log solubility in mols per litre\n",
      "computed_metrics: [0.94448420122904553]\n",
      "computed_metrics: [0.66876784364694708]\n",
      "computed_metrics: [0.51629442708305984]\n",
      "({'mean-pearson_r2_score': 0.94448420122904553}, {'mean-pearson_r2_score': 0.66876784364694708}, {'mean-pearson_r2_score': 0.51629442708305984})\n",
      "t = 2.8410081863\n",
      "================================================================================\n",
      "================================================================================\n",
      "Dataset: delaney, split: random\n",
      "-------------------------------------\n",
      "Loading dataset: delaney\n",
      "-------------------------------------\n",
      "Splitting function: random\n",
      "About to featurize Delaney dataset.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/delaney-processed.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 49.728 s\n",
      "TIMING: dataset construction took 49.883 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 0.175 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.144 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.038 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.040 s\n",
      "Loading dataset from disk.\n",
      "About to initialize singletask to multitask model\n",
      "Initializing directory for task measured log solubility in mols per litre\n",
      "About to create task-specific datasets\n",
      "Splitting multitask dataset into singletask datasets\n",
      "TIMING: dataset construction took 0.001 s\n",
      "Loading dataset from disk.\n",
      "Processing shard 0\n",
      "\tTask measured log solubility in mols per litre\n",
      "Dataset for task measured log solubility in mols per litre has shape ((902, 1024), (902, 1), (902, 1), (902,))\n",
      "Fitting model for task measured log solubility in mols per litre\n",
      "computed_metrics: [0.93810698172594742]\n",
      "computed_metrics: [0.60317635661390367]\n",
      "computed_metrics: [0.62178963466087722]\n",
      "({'mean-pearson_r2_score': 0.93810698172594742}, {'mean-pearson_r2_score': 0.60317635661390367}, {'mean-pearson_r2_score': 0.62178963466087722})\n",
      "t = 2.8439941406\n",
      "================================================================================\n",
      "================================================================================\n",
      "Dataset: delaney, split: scaffold\n",
      "-------------------------------------\n",
      "Loading dataset: delaney\n",
      "-------------------------------------\n",
      "Splitting function: scaffold\n",
      "About to featurize Delaney dataset.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/delaney-processed.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 49.685 s\n",
      "TIMING: dataset construction took 49.841 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 0.175 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.145 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.038 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.039 s\n",
      "Loading dataset from disk.\n",
      "About to initialize singletask to multitask model\n",
      "Initializing directory for task measured log solubility in mols per litre\n",
      "About to create task-specific datasets\n",
      "Splitting multitask dataset into singletask datasets\n",
      "TIMING: dataset construction took 0.001 s\n",
      "Loading dataset from disk.\n",
      "Processing shard 0\n",
      "\tTask measured log solubility in mols per litre\n",
      "Dataset for task measured log solubility in mols per litre has shape ((902, 1024), (902, 1), (902, 1), (902,))\n",
      "Fitting model for task measured log solubility in mols per litre\n",
      "computed_metrics: [0.94743324181477984]\n",
      "computed_metrics: [0.54441442570895149]\n",
      "computed_metrics: [0.54346957507725302]\n",
      "({'mean-pearson_r2_score': 0.94743324181477984}, {'mean-pearson_r2_score': 0.54441442570895149}, {'mean-pearson_r2_score': 0.54346957507725302})\n",
      "t = 2.7897939682\n",
      "================================================================================\n",
      "================================================================================\n",
      "Dataset: sampl, split: index\n",
      "-------------------------------------\n",
      "Loading dataset: sampl\n",
      "-------------------------------------\n",
      "Splitting function: index\n",
      "About to featurize SAMPL dataset.\n",
      "About to load MUV dataset.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/SAMPL.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 20.627 s\n",
      "TIMING: dataset construction took 20.716 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 0.101 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.085 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.022 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.022 s\n",
      "Loading dataset from disk.\n",
      "About to initialize singletask to multitask model\n",
      "Initializing directory for task expt\n",
      "About to create task-specific datasets\n",
      "Splitting multitask dataset into singletask datasets\n",
      "TIMING: dataset construction took 0.001 s\n",
      "Loading dataset from disk.\n",
      "Processing shard 0\n",
      "\tTask expt\n",
      "Dataset for task expt has shape ((514, 1024), (514, 1), (514, 1), (514,))\n",
      "Fitting model for task expt\n",
      "computed_metrics: [0.93647812897873484]\n",
      "computed_metrics: [0.34471387480586863]\n",
      "computed_metrics: [0.63188951260318715]\n",
      "({'mean-pearson_r2_score': 0.93647812897873484}, {'mean-pearson_r2_score': 0.34471387480586863}, {'mean-pearson_r2_score': 0.63188951260318715})\n",
      "t = 1.4533848763\n",
      "================================================================================\n",
      "================================================================================\n",
      "Dataset: sampl, split: random\n",
      "-------------------------------------\n",
      "Loading dataset: sampl\n",
      "-------------------------------------\n",
      "Splitting function: random\n",
      "About to featurize SAMPL dataset.\n",
      "About to load MUV dataset.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/SAMPL.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 20.501 s\n",
      "TIMING: dataset construction took 20.590 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 0.103 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.082 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.022 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.022 s\n",
      "Loading dataset from disk.\n",
      "About to initialize singletask to multitask model\n",
      "Initializing directory for task expt\n",
      "About to create task-specific datasets\n",
      "Splitting multitask dataset into singletask datasets\n",
      "TIMING: dataset construction took 0.001 s\n",
      "Loading dataset from disk.\n",
      "Processing shard 0\n",
      "\tTask expt\n",
      "Dataset for task expt has shape ((514, 1024), (514, 1), (514, 1), (514,))\n",
      "Fitting model for task expt\n",
      "computed_metrics: [0.92804336100873375]\n",
      "computed_metrics: [0.52560024868333433]\n",
      "computed_metrics: [0.60930886028759723]\n",
      "({'mean-pearson_r2_score': 0.92804336100873375}, {'mean-pearson_r2_score': 0.52560024868333433}, {'mean-pearson_r2_score': 0.60930886028759723})\n",
      "t = 1.4439690113\n",
      "================================================================================\n",
      "================================================================================\n",
      "Dataset: sampl, split: scaffold\n",
      "-------------------------------------\n",
      "Loading dataset: sampl\n",
      "-------------------------------------\n",
      "Splitting function: scaffold\n",
      "About to featurize SAMPL dataset.\n",
      "About to load MUV dataset.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from /tmp/SAMPL.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 20.430 s\n",
      "TIMING: dataset construction took 20.520 s\n",
      "Loading dataset from disk.\n",
      "About to transform data\n",
      "TIMING: dataset construction took 0.101 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.082 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.025 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.024 s\n",
      "Loading dataset from disk.\n",
      "About to initialize singletask to multitask model\n",
      "Initializing directory for task expt\n",
      "About to create task-specific datasets\n",
      "Splitting multitask dataset into singletask datasets\n",
      "TIMING: dataset construction took 0.001 s\n",
      "Loading dataset from disk.\n",
      "Processing shard 0\n",
      "\tTask expt\n",
      "Dataset for task expt has shape ((514, 1024), (514, 1), (514, 1), (514,))\n",
      "Fitting model for task expt\n",
      "computed_metrics: [0.92621163407191043]\n",
      "computed_metrics: [0.54980201372197168]\n",
      "computed_metrics: [0.35005217569673097]\n",
      "({'mean-pearson_r2_score': 0.92621163407191043}, {'mean-pearson_r2_score': 0.54980201372197168}, {'mean-pearson_r2_score': 0.35005217569673097})\n",
      "t = 1.4496138096\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from deepchem.molnet.run_benchmark import load_dataset, benchmark_model\n",
    "from itertools import product\n",
    "\n",
    "metric = [dc.metrics.Metric(dc.metrics.pearson_r2_score, np.mean)]\n",
    "\n",
    "datasets = [\n",
    "    # 'kaggle', # does not allow featurizer input\n",
    "    'delaney', # we discard it here to save the testing time.\n",
    "    # 'nci', # Could be very large. discard it if we do not use it.\n",
    "    # 'chembl', # too long to run, discard.\n",
    "    'sampl'\n",
    "]\n",
    "splits = [\n",
    "    \"index\",\n",
    "    \"random\",\n",
    "    \"scaffold\"\n",
    "]\n",
    "\n",
    "for dataset, split in product(datasets, splits):\n",
    "    print(\"=\"*80)\n",
    "    print(\"Dataset: %s, split: %s\" % (dataset, split))\n",
    "    tasks, all_datasets, transformers = load_dataset(dataset, featurizer, split)\n",
    "    reg_model = dc.models.multitask.SingletaskToMultitask(tasks, SKLearnModelSelector(dataset, split))\n",
    "    train, val, test, t = benchmark_model(reg_model, all_datasets, transformers, metric, test=True)\n",
    "    print(train, val, test)\n",
    "    print(\"t = %.10f\" % t)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
